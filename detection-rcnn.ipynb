{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9255939,"sourceType":"datasetVersion","datasetId":5600029},{"sourceId":9292177,"sourceType":"datasetVersion","datasetId":5625450},{"sourceId":9293216,"sourceType":"datasetVersion","datasetId":5626240}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"root = \"/kaggle/input/d/lawliet07/rnd-beg-dataset/test_images/\"","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:16.190323Z","iopub.execute_input":"2024-09-01T16:09:16.190783Z","iopub.status.idle":"2024-09-01T16:09:16.195775Z","shell.execute_reply.started":"2024-09-01T16:09:16.190736Z","shell.execute_reply":"2024-09-01T16:09:16.194662Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nlen(os.listdir(root))","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:18.902673Z","iopub.execute_input":"2024-09-01T16:09:18.903190Z","iopub.status.idle":"2024-09-01T16:09:18.922230Z","shell.execute_reply.started":"2024-09-01T16:09:18.903136Z","shell.execute_reply":"2024-09-01T16:09:18.920983Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"45"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nimport torchvision.transforms as transforms\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:21.110871Z","iopub.execute_input":"2024-09-01T16:09:21.111325Z","iopub.status.idle":"2024-09-01T16:09:24.359848Z","shell.execute_reply.started":"2024-09-01T16:09:21.111278Z","shell.execute_reply":"2024-09-01T16:09:24.358690Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# transform = torchvision.transforms.Compose([\n#     transforms.ToTensor()\n# ])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:33:20.680742Z","iopub.execute_input":"2024-09-01T09:33:20.682507Z","iopub.status.idle":"2024-09-01T09:33:20.696054Z","shell.execute_reply.started":"2024-09-01T09:33:20.682447Z","shell.execute_reply":"2024-09-01T09:33:20.694103Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"dataset_paths = os.listdir(root)\ndataset_paths = [root + img for img in dataset_paths]\ndataset_paths[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:28.583047Z","iopub.execute_input":"2024-09-01T16:09:28.584286Z","iopub.status.idle":"2024-09-01T16:09:28.595092Z","shell.execute_reply.started":"2024-09-01T16:09:28.584236Z","shell.execute_reply":"2024-09-01T16:09:28.593832Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/d/lawliet07/rnd-beg-dataset/test_images/test_buddha2.jpg',\n '/kaggle/input/d/lawliet07/rnd-beg-dataset/test_images/test_buddha27.jpg',\n '/kaggle/input/d/lawliet07/rnd-beg-dataset/test_images/test_buddha41.jpg',\n '/kaggle/input/d/lawliet07/rnd-beg-dataset/test_images/test_buddha45.jpg',\n '/kaggle/input/d/lawliet07/rnd-beg-dataset/test_images/test_buddha11.jpg']"},"metadata":{}}]},{"cell_type":"code","source":"label_map = {\"broken\" : 0,\n             \"buddha\" : 1}","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:31.988055Z","iopub.execute_input":"2024-09-01T16:09:31.988482Z","iopub.status.idle":"2024-09-01T16:09:31.993788Z","shell.execute_reply.started":"2024-09-01T16:09:31.988439Z","shell.execute_reply":"2024-09-01T16:09:31.992482Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import xml.etree.ElementTree as ET\nannotation_path = \"/kaggle/input/buddha-annotations-coco/\"\nanno_data = os.listdir(annotation_path)\nanno_data = [annotation_path + anno for anno in anno_data]\nboxes = []\nlabels = []\nfor annotation in anno_data:\n    tree = ET.parse(annotation)\n    root = tree.getroot()\n    all_box = []\n    all_label = []\n    for obj in root.findall(\"object\"):\n            bbox = obj.find(\"bndbox\")\n            xmin = int(bbox.find(\"xmin\").text)\n            ymin = int(bbox.find(\"ymin\").text)\n            xmax = int(bbox.find(\"xmax\").text)\n            ymax = int(bbox.find(\"ymax\").text)\n            all_box.append([xmin, ymin, xmax, ymax])\n            label = obj.find(\"name\").text\n            all_label.append(label_map[label])\n    boxes.append(torch.as_tensor(all_box))\n    labels.append(torch.as_tensor(all_label))\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:35.043748Z","iopub.execute_input":"2024-09-01T16:09:35.044194Z","iopub.status.idle":"2024-09-01T16:09:35.240550Z","shell.execute_reply.started":"2024-09-01T16:09:35.044151Z","shell.execute_reply":"2024-09-01T16:09:35.239523Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# labels[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:33:20.785741Z","iopub.execute_input":"2024-09-01T09:33:20.786216Z","iopub.status.idle":"2024-09-01T09:33:20.793640Z","shell.execute_reply.started":"2024-09-01T09:33:20.786170Z","shell.execute_reply":"2024-09-01T09:33:20.792002Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"target = []\n# boxes = torch.as_tensor(boxes)\n# labels = torch.as_tensor(labels)\nfor i in range(len(boxes)):\n    target.append({\"boxes\": boxes[i], \"labels\": labels[i]})","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:39.021832Z","iopub.execute_input":"2024-09-01T16:09:39.022579Z","iopub.status.idle":"2024-09-01T16:09:39.028481Z","shell.execute_reply.started":"2024-09-01T16:09:39.022533Z","shell.execute_reply":"2024-09-01T16:09:39.027099Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"boxes[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:44.760190Z","iopub.execute_input":"2024-09-01T16:09:44.760634Z","iopub.status.idle":"2024-09-01T16:09:44.793969Z","shell.execute_reply.started":"2024-09-01T16:09:44.760577Z","shell.execute_reply":"2024-09-01T16:09:44.792867Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"tensor([[ 71,  25, 110,  94],\n        [106,   0, 159,  27],\n        [152,  29, 187, 101],\n        [ 23,   0, 243, 152]])"},"metadata":{}}]},{"cell_type":"code","source":"target[0]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:48.238682Z","iopub.execute_input":"2024-09-01T16:09:48.239141Z","iopub.status.idle":"2024-09-01T16:09:48.247979Z","shell.execute_reply.started":"2024-09-01T16:09:48.239089Z","shell.execute_reply":"2024-09-01T16:09:48.246808Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'boxes': tensor([[ 71,  25, 110,  94],\n         [106,   0, 159,  27],\n         [152,  29, 187, 101],\n         [ 23,   0, 243, 152]]),\n 'labels': tensor([0, 0, 0, 1])}"},"metadata":{}}]},{"cell_type":"code","source":"dataset = []\nfor img_path in dataset_paths:\n    img = Image.open(img_path).convert(\"RGB\")\n    img = transforms.Resize((512, 512))(img)\n    img = transforms.ToTensor()(img)\n    dataset.append(img)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:09:59.391337Z","iopub.execute_input":"2024-09-01T16:09:59.391817Z","iopub.status.idle":"2024-09-01T16:10:00.057704Z","shell.execute_reply.started":"2024-09-01T16:09:59.391771Z","shell.execute_reply":"2024-09-01T16:10:00.056378Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# len(dataset[0])\n(dataset[0]).shape","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:10:02.791963Z","iopub.execute_input":"2024-09-01T16:10:02.792955Z","iopub.status.idle":"2024-09-01T16:10:02.799880Z","shell.execute_reply.started":"2024-09-01T16:10:02.792905Z","shell.execute_reply":"2024-09-01T16:10:02.798677Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 512, 512])"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(len(dataset)):\n    dataset[i] = [dataset[i], target[i]]\n# dataset[:5]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:15:07.660564Z","iopub.execute_input":"2024-09-01T16:15:07.661575Z","iopub.status.idle":"2024-09-01T16:15:07.666989Z","shell.execute_reply.started":"2024-09-01T16:15:07.661528Z","shell.execute_reply":"2024-09-01T16:15:07.665699Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:12:03.578249Z","iopub.execute_input":"2024-09-01T16:12:03.578717Z","iopub.status.idle":"2024-09-01T16:12:03.584024Z","shell.execute_reply.started":"2024-09-01T16:12:03.578672Z","shell.execute_reply":"2024-09-01T16:12:03.582889Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data_loader = DataLoader(\n    dataset,\n    batch_size=2,  # You can adjust this as needed\n    shuffle=True,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:12:06.555099Z","iopub.execute_input":"2024-09-01T16:12:06.555501Z","iopub.status.idle":"2024-09-01T16:12:06.561291Z","shell.execute_reply.started":"2024-09-01T16:12:06.555462Z","shell.execute_reply":"2024-09-01T16:12:06.559889Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define Faster R-CNN model\nbackbone = torchvision.models.resnet50(pretrained=True)\nbackbone.out_channels = 2048  # Set output channels for ResNet backbone\nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                 output_size=7,\n                                                 sampling_ratio=2)\nmodel = FasterRCNN(backbone,\n                   num_classes=3,  # Adjust based on your dataset's number of classes\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)\n\n# Define optimizer and learning rate scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n# Training loop (simplified)\n# model.train()\n# num_epochs = 10\n# for epoch in range(num_epochs):\n#     for images, targets in data_loader:\n#         optimizer.zero_grad()\n#         loss_dict = model(images, targets)\n#         losses = sum(loss for loss in loss_dict.values())\n#         losses.backward()\n#         optimizer.step()\n\n#     lr_scheduler.step()\n#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item()}\")\n\n# # Save the trained model\n# torch.save(model.state_dict(), \"faster_rcnn_buddha_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:12:11.263998Z","iopub.execute_input":"2024-09-01T16:12:11.264415Z","iopub.status.idle":"2024-09-01T16:12:14.544016Z","shell.execute_reply.started":"2024-09-01T16:12:11.264376Z","shell.execute_reply":"2024-09-01T16:12:14.542607Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 151MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"model.train()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for images, targets in data_loader:\n        # Convert images and targets to lists for the batch\n        images = list(image for image in images)\n        targets = [{k: v for k, v in t.items()} for t in targets]\n\n        optimizer.zero_grad()\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        losses.backward()\n        optimizer.step()\n\n    lr_scheduler.step()\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-01T16:15:13.712944Z","iopub.execute_input":"2024-09-01T16:15:13.713389Z","iopub.status.idle":"2024-09-01T16:15:14.275824Z","shell.execute_reply.started":"2024-09-01T16:15:13.713348Z","shell.execute_reply":"2024-09-01T16:15:14.274162Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     12\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:76\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m original_image_sizes: List[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m---> 76\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting the last two dimensions of the Tensor to be H and W instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     original_image_sizes\u001b[38;5;241m.\u001b[39mappend((val[\u001b[38;5;241m0\u001b[39m], val[\u001b[38;5;241m1\u001b[39m]))\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","output_type":"error"}]},{"cell_type":"code","source":" torch.save(model.state_dict(), \"faster_rcnn_buddha_model.pth\")","metadata":{},"execution_count":null,"outputs":[]}]}